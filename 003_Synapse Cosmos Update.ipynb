{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 Spark - Cosmos Write and Update from \r\n",
        "\r\n",
        "https://github.com/Azure/azure-sdk-for-java/blob/main/sdk/cosmos/azure-cosmos-spark_3_2-12/Samples/Python/NYC-Taxi-Data/01_Batch.ipynb\r\n",
        "\r\n",
        "This IDs there are dynamically set every time, so updates will fail, updated code:\r\n",
        "\r\n",
        "```\r\n",
        "df_input_withId = df_rawInput \\\r\n",
        "  .withColumn(\"id\", f.expr(\"uuid()\")) \\\r\n",
        "  ```\r\n",
        "\r\n",
        "  I take 100 records and change the vendorId to be 0 and update those records"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cosmosEndpoint = 'https://mmh.documents.azure.com:443/'\r\n",
        "cosmosMasterKey = 'MyMasterKey=='\r\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "pool02",
              "session_id": 3,
              "statement_id": 8,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-05-04T22:29:13.6493214Z",
              "session_start_time": null,
              "execution_start_time": "2022-05-04T22:29:13.7681836Z",
              "execution_finish_time": "2022-05-04T22:29:13.7683933Z"
            },
            "text/plain": "StatementMeta(pool02, 3, 8, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 295,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import uuid\r\n",
        "import pyspark.sql.functions as f\r\n",
        "spark.conf.set(\"spark.sql.catalog.cosmosCatalog\", \"com.azure.cosmos.spark.CosmosCatalog\")\r\n",
        "spark.conf.set(\"spark.sql.catalog.cosmosCatalog.spark.cosmos.accountEndpoint\", cosmosEndpoint)\r\n",
        "spark.conf.set(\"spark.sql.catalog.cosmosCatalog.spark.cosmos.accountKey\", cosmosMasterKey)\r\n",
        "spark.conf.set(\"spark.sql.catalog.cosmosCatalog.spark.cosmos.views.repositoryPath\", \"/viewDefinitions\" + str(uuid.uuid4()))"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "pool02",
              "session_id": 3,
              "statement_id": 9,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-05-04T22:29:14.6601567Z",
              "session_start_time": null,
              "execution_start_time": "2022-05-04T22:29:14.845465Z",
              "execution_finish_time": "2022-05-04T22:29:14.9869892Z"
            },
            "text/plain": "StatementMeta(pool02, 3, 9, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 296,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%sql\r\n",
        "CREATE DATABASE IF NOT EXISTS cosmosCatalog.SampleDatabase;\r\n",
        "\r\n",
        "CREATE TABLE IF NOT EXISTS cosmosCatalog.SampleDatabase.GreenTaxiRecords\r\n",
        "USING cosmos.oltp\r\n",
        "TBLPROPERTIES(partitionKeyPath = '/id', autoScaleMaxThroughput = '100000', indexingPolicy = 'OnlySystemProperties');\r\n",
        "\r\n",
        "CREATE TABLE IF NOT EXISTS cosmosCatalog.SampleDatabase.GreenTaxiRecordsCFSink\r\n",
        "USING cosmos.oltp\r\n",
        "TBLPROPERTIES(partitionKeyPath = '/id', autoScaleMaxThroughput = '100000', indexingPolicy = 'OnlySystemProperties');\r\n",
        "\r\n",
        "/* NOTE: It is important to enable TTL (can be off/-1 by default) on the throughput control container */\r\n",
        "CREATE TABLE IF NOT EXISTS cosmosCatalog.SampleDatabase.ThroughputControl\r\n",
        "USING cosmos.oltp\r\n",
        "OPTIONS(spark.cosmos.database = 'SampleDatabase')\r\n",
        "TBLPROPERTIES(partitionKeyPath = '/groupId', autoScaleMaxThroughput = '4000', indexingPolicy = 'AllProperties', defaultTtlInSeconds = '-1');"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": null,
              "session_id": 3,
              "statement_id": -1,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-05-04T22:29:17.1338271Z",
              "session_start_time": null,
              "execution_start_time": "2022-05-04T22:29:26.2346185Z",
              "execution_finish_time": "2022-05-04T22:29:26.2348266Z"
            },
            "text/plain": "StatementMeta(, 3, -1, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 297,
          "data": {
            "application/vnd.synapse.sparksql-result+json": {
              "schema": {
                "type": "struct",
                "fields": []
              },
              "data": []
            },
            "text/plain": "<Spark SQL result set with 0 rows and 0 fields>"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 297,
          "data": {
            "application/vnd.synapse.sparksql-result+json": {
              "schema": {
                "type": "struct",
                "fields": []
              },
              "data": []
            },
            "text/plain": "<Spark SQL result set with 0 rows and 0 fields>"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 297,
          "data": {
            "application/vnd.synapse.sparksql-result+json": {
              "schema": {
                "type": "struct",
                "fields": []
              },
              "data": []
            },
            "text/plain": "<Spark SQL result set with 0 rows and 0 fields>"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 297,
          "data": {
            "application/vnd.synapse.sparksql-result+json": {
              "schema": {
                "type": "struct",
                "fields": []
              },
              "data": []
            },
            "text/plain": "<Spark SQL result set with 0 rows and 0 fields>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 297,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\r\n",
        "import time\r\n",
        "import uuid\r\n",
        "from pyspark.sql.functions import udf\r\n",
        "from pyspark.sql.types import StringType, LongType\r\n",
        "import pyspark.sql.functions as f\r\n",
        "\r\n",
        "print(\"Starting preparation: \", datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\"))\r\n",
        "# Azure storage access info\r\n",
        "blob_account_name = \"azureopendatastorage\"\r\n",
        "blob_container_name = \"nyctlc\"\r\n",
        "blob_relative_path = \"green\"\r\n",
        "blob_sas_token = r\"\"\r\n",
        "# Allow SPARK to read from Blob remotely\r\n",
        "wasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\r\n",
        "spark.conf.set(\r\n",
        "  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\r\n",
        "  blob_sas_token)\r\n",
        "print('Remote blob path: ' + wasbs_path)\r\n",
        "# SPARK read parquet, note that it won't load any data yet by now\r\n",
        "# NOTE - if you want to experiment with larger dataset sizes - consider switching to Option B (commenting code \r\n",
        "# for Option A/uncommenting code for option B) the lines below or increase the value passed into the \r\n",
        "# limit function restricting the dataset size below\r\n",
        "\r\n",
        "#------------------------------------------------------------------------------------\r\n",
        "# Option A - with limited dataset size\r\n",
        "#------------------------------------------------------------------------------------\r\n",
        "df_rawInputWithoutLimit = spark.read.parquet(wasbs_path)\r\n",
        "partitionCount = df_rawInputWithoutLimit.rdd.getNumPartitions()\r\n",
        "df_rawInput = df_rawInputWithoutLimit.limit(1_000_000).repartition(partitionCount) #1_000_000\r\n",
        "df_rawInput.persist()\r\n",
        "\r\n",
        "#------------------------------------------------------------------------------------\r\n",
        "# Option B - entire dataset\r\n",
        "#------------------------------------------------------------------------------------\r\n",
        "#df_rawInput = spark.read.parquet(wasbs_path)\r\n",
        "\r\n",
        "\r\n",
        "nowUdf= udf(lambda : int(time.time() * 1000),LongType())\r\n",
        "df_input_withId = df_rawInput \\\r\n",
        "  .withColumn(\"id\", f.expr(\"uuid()\")) \\\r\n",
        "  .withColumn(\"insertedAt\", nowUdf()) \\\r\n",
        "\r\n",
        "print('Register the DataFrame as a SQL temporary view: source')\r\n",
        "df_input_withId.createOrReplaceTempView('source')\r\n",
        "print(\"Finished preparation: \", datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\"))\r\n",
        "\r\n",
        "df = df_input_withId"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "pool02",
              "session_id": 3,
              "statement_id": 14,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-05-04T22:29:18.4762341Z",
              "session_start_time": null,
              "execution_start_time": "2022-05-04T22:29:26.3479938Z",
              "execution_finish_time": "2022-05-04T22:29:33.1725822Z"
            },
            "text/plain": "StatementMeta(pool02, 3, 14, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting preparation:  2022-05-04 22:29:26.325303\nRemote blob path: wasbs://nyctlc@azureopendatastorage.blob.core.windows.net/green\nRegister the DataFrame as a SQL temporary view: source\nFinished preparation:  2022-05-04 22:29:32.132500"
          ]
        }
      ],
      "execution_count": 298,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Write Configuration - bulk.enabled with ItemOverwrite"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import uuid\r\n",
        "import datetime\r\n",
        "\r\n",
        "print(\"Starting ingestion: \", datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\"))\r\n",
        "\r\n",
        "writeCfg = {\r\n",
        "  \"spark.cosmos.accountEndpoint\": cosmosEndpoint,\r\n",
        "  \"spark.cosmos.accountKey\": cosmosMasterKey,\r\n",
        "  \"spark.cosmos.database\": \"SampleDatabase\",\r\n",
        "  \"spark.cosmos.container\": \"GreenTaxiRecords\",\r\n",
        "  \"spark.cosmos.write.strategy\": \"ItemOverwrite\",\r\n",
        "  \"spark.cosmos.write.bulk.enabled\": \"true\",\r\n",
        "  \"spark.cosmos.throughputControl.enabled\": \"true\",\r\n",
        "  \"spark.cosmos.throughputControl.name\": \"NYCGreenTaxiDataIngestion\",\r\n",
        "  \"spark.cosmos.throughputControl.targetThroughputThreshold\": \"0.95\",\r\n",
        "  \"spark.cosmos.throughputControl.globalControl.database\": \"SampleDatabase\",\r\n",
        "  \"spark.cosmos.throughputControl.globalControl.container\": \"ThroughputControl\",\r\n",
        "}\r\n",
        "\r\n",
        "\r\n",
        "df_input_withId \\\r\n",
        "  .write \\\r\n",
        "  .format(\"cosmos.oltp\") \\\r\n",
        "  .mode(\"Append\") \\\r\n",
        "  .options(**writeCfg) \\\r\n",
        "  .save()\r\n",
        "\r\n",
        "print(\"Finished ingestion: \", datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\"))"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "pool02",
              "session_id": 3,
              "statement_id": 15,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-05-04T22:29:39.0512756Z",
              "session_start_time": null,
              "execution_start_time": "2022-05-04T22:29:39.1904155Z",
              "execution_finish_time": "2022-05-04T22:31:56.3957116Z"
            },
            "text/plain": "StatementMeta(pool02, 3, 15, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting ingestion:  2022-05-04 22:29:39.166637\nFinished ingestion:  2022-05-04 22:31:55.436754"
          ]
        }
      ],
      "execution_count": 299,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "count_source = spark.sql('SELECT * FROM source').count()\r\n",
        "print(\"Number of records in source: \", count_source) "
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "pool02",
              "session_id": 3,
              "statement_id": 16,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-05-04T22:32:57.5868242Z",
              "session_start_time": null,
              "execution_start_time": "2022-05-04T22:32:57.726606Z",
              "execution_finish_time": "2022-05-04T22:32:58.7748677Z"
            },
            "text/plain": "StatementMeta(pool02, 3, 16, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of records in source:  1000000"
          ]
        }
      ],
      "execution_count": 300,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read from Cosmos"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read from Cosmos"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import *\r\n",
        "import pyspark.sql.functions as F\r\n",
        "\r\n",
        "print(\"Starting validation via query: \", datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\"))\r\n",
        "readCfg = {\r\n",
        "  \"spark.cosmos.accountEndpoint\": cosmosEndpoint,\r\n",
        "  \"spark.cosmos.accountKey\": cosmosMasterKey,\r\n",
        "  \"spark.cosmos.database\": \"SampleDatabase\",\r\n",
        "  \"spark.cosmos.container\": \"GreenTaxiRecords\",\r\n",
        "  \"spark.cosmos.read.partitioning.strategy\": \"Restrictive\",#IMPORTANT - any other partitioning strategy will result in indexing not being use to count - so latency and RU would spike up\r\n",
        "  \"spark.cosmos.read.inferSchema.enabled\" : \"true\",\r\n",
        "  \"spark.cosmos.read.customQuery\" : \"SELECT COUNT(0) AS Count FROM c\"\r\n",
        "}\r\n",
        "\r\n",
        "count_query_schema=StructType(fields=[StructField(\"Count\", LongType(), True)])\r\n",
        "query_df = spark.read.format(\"cosmos.oltp\").schema(count_query_schema).options(**readCfg).load()\r\n",
        "count_query = query_df.select(F.sum(\"Count\").alias(\"TotalCount\")).first()[\"TotalCount\"]\r\n",
        "print(\"Number of records retrieved via query: \", count_query) \r\n",
        "print(\"Finished validation via query: \", datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\"))\r\n",
        "\r\n",
        "assert count_source == count_query"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "pool02",
              "session_id": 3,
              "statement_id": 17,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-05-04T22:33:45.5951955Z",
              "session_start_time": null,
              "execution_start_time": "2022-05-04T22:33:45.7355592Z",
              "execution_finish_time": "2022-05-04T22:33:51.0174726Z"
            },
            "text/plain": "StatementMeta(pool02, 3, 17, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting validation via query:  2022-05-04 22:33:45.698941\nNumber of records retrieved via query:  1000000\nFinished validation via query:  2022-05-04 22:33:50.464621"
          ]
        }
      ],
      "execution_count": 301,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\r\n",
        "\r\n",
        "print(\"identify documents: \", datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\"))\r\n",
        "readCfgwithRaw = {\r\n",
        "  \"spark.cosmos.accountEndpoint\": cosmosEndpoint,\r\n",
        "  \"spark.cosmos.accountKey\": cosmosMasterKey,\r\n",
        "  \"spark.cosmos.database\": \"SampleDatabase\",\r\n",
        "  \"spark.cosmos.container\": \"GreenTaxiRecords\",\r\n",
        "  \"spark.cosmos.read.partitioning.strategy\": \"Default\",\r\n",
        "  \"spark.cosmos.read.inferSchema.enabled\" : \"true\",\r\n",
        "}\r\n",
        "\r\n",
        "cosmos_df = spark.read.format(\"cosmos.oltp\").options(**readCfgwithRaw).load()\r\n",
        "print(cosmos_df.count())\r\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "pool02",
              "session_id": 3,
              "statement_id": 18,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-05-04T22:33:50.2434998Z",
              "session_start_time": null,
              "execution_start_time": "2022-05-04T22:33:51.1540598Z",
              "execution_finish_time": "2022-05-04T22:34:40.6190462Z"
            },
            "text/plain": "StatementMeta(pool02, 3, 18, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "identify documents:  2022-05-04 22:33:51.128751\n1000000"
          ]
        }
      ],
      "execution_count": 302,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_to_update = df.limit(100)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "pool02",
              "session_id": 3,
              "statement_id": 19,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-05-04T22:34:47.6960584Z",
              "session_start_time": null,
              "execution_start_time": "2022-05-04T22:34:47.8211651Z",
              "execution_finish_time": "2022-05-04T22:34:47.9714351Z"
            },
            "text/plain": "StatementMeta(pool02, 3, 19, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 303,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_to_update = df_to_update.withColumn('vendorId', f.lit(0))"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "pool02",
              "session_id": 3,
              "statement_id": 20,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-05-04T22:34:48.7808023Z",
              "session_start_time": null,
              "execution_start_time": "2022-05-04T22:34:48.8993238Z",
              "execution_finish_time": "2022-05-04T22:34:49.392101Z"
            },
            "text/plain": "StatementMeta(pool02, 3, 20, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 304,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\r\n",
        "\r\n",
        "print(\"Number of records to be updated: \", df.count()) \r\n",
        "\r\n",
        "print(\"Starting to bulk update documents: \", datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\"))\r\n",
        "UpdateCfg = writeCfg.copy()\r\n",
        "\r\n",
        "df_to_update \\\r\n",
        "        .write \\\r\n",
        "        .format(\"cosmos.oltp\") \\\r\n",
        "        .mode(\"Append\") \\\r\n",
        "        .options(**UpdateCfg) \\\r\n",
        "        .save()\r\n",
        "print(\"Finished updating documents: \", datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\"))\r\n",
        "\r\n",
        "print(\"Starting count validation via query: \", datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\"))\r\n",
        "count_query_schema=StructType(fields=[StructField(\"Count\", LongType(), True)])\r\n",
        "readCfg[\"spark.cosmos.read.customQuery\"] = \"SELECT COUNT(0) AS Count FROM c\"\r\n",
        "query_df = spark.read.format(\"cosmos.oltp\").schema(count_query_schema).options(**readCfg).load()\r\n",
        "count_query = query_df.select(F.sum(\"Count\").alias(\"TotalCount\")).first()[\"TotalCount\"]\r\n",
        "print(\"Number of records retrieved via query: \", count_query) \r\n",
        "print(\"Finished count validation via query: \", datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\"))\r\n",
        "\r\n",
        "print(count_query)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "pool02",
              "session_id": 3,
              "statement_id": 21,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-05-04T22:34:49.4457256Z",
              "session_start_time": null,
              "execution_start_time": "2022-05-04T22:34:49.5579431Z",
              "execution_finish_time": "2022-05-04T22:34:54.8439665Z"
            },
            "text/plain": "StatementMeta(pool02, 3, 21, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of records to be updated:  1000000\nStarting to bulk update documents:  2022-05-04 22:34:49.854036\nFinished updating documents:  2022-05-04 22:34:53.930036\nStarting count validation via query:  2022-05-04 22:34:53.930300\nNumber of records retrieved via query:  1000000\nFinished count validation via query:  2022-05-04 22:34:54.691137\n1000000"
          ]
        }
      ],
      "execution_count": 305,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check where your vendor ID is 0, there should be 100 rows"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Starting count validation via query: \", datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\"))\r\n",
        "count_query_schema=StructType(fields=[StructField(\"Count\", LongType(), True)])\r\n",
        "readCfg[\"spark.cosmos.read.customQuery\"] = \"SELECT COUNT(0) AS Count FROM c where c.vendorId = 0\"\r\n",
        "query_df = spark.read.format(\"cosmos.oltp\").schema(count_query_schema).options(**readCfg).load()\r\n",
        "count_query = query_df.select(F.sum(\"Count\").alias(\"TotalCount\")).first()[\"TotalCount\"]\r\n",
        "print(\"Number of records retrieved via query: \", count_query) \r\n",
        "print(\"Finished count validation via query: \", datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\"))\r\n",
        "\r\n",
        "print(count_query)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "pool02",
              "session_id": 3,
              "statement_id": 22,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-05-04T22:34:58.3973539Z",
              "session_start_time": null,
              "execution_start_time": "2022-05-04T22:34:58.5101272Z",
              "execution_finish_time": "2022-05-04T22:34:59.5503848Z"
            },
            "text/plain": "StatementMeta(pool02, 3, 22, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting count validation via query:  2022-05-04 22:34:58.486932\nNumber of records retrieved via query:  100\nFinished count validation via query:  2022-05-04 22:34:59.486427\n100"
          ]
        }
      ],
      "execution_count": 306,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}