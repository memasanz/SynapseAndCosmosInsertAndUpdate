{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\r\n",
        "cosmosEndpoint = 'https://mm.documents.azure.com:443/'\r\n",
        "cosmosMasterKey = 'MyMasterKey'"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "pool03",
              "session_id": 0,
              "statement_id": 3,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-05-04T22:58:04.5917829Z",
              "session_start_time": null,
              "execution_start_time": "2022-05-04T22:58:04.7155505Z",
              "execution_finish_time": "2022-05-04T22:58:04.7159459Z"
            },
            "text/plain": "StatementMeta(pool03, 0, 3, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 2,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "\r\n",
        "cosmosDatabaseName = \"sampleDB\"\r\n",
        "cosmosContainerName = \"sampleContainer\"\r\n",
        "\r\n",
        "cfg = {\r\n",
        "  \"spark.cosmos.accountEndpoint\" : cosmosEndpoint,\r\n",
        "  \"spark.cosmos.accountKey\" : cosmosMasterKey,\r\n",
        "  \"spark.cosmos.database\" : cosmosDatabaseName,\r\n",
        "  \"spark.cosmos.container\" : cosmosContainerName,\r\n",
        "  \"spark.cosmos.write.strategy\": \"ItemOverwrite\",\r\n",
        "  \"spark.cosmos.write.bulk.enabled\": \"true\",\r\n",
        "}"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "pool03",
              "session_id": 0,
              "statement_id": 4,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-05-04T22:58:05.6254041Z",
              "session_start_time": null,
              "execution_start_time": "2022-05-04T22:58:05.7674018Z",
              "execution_finish_time": "2022-05-04T22:58:05.927511Z"
            },
            "text/plain": "StatementMeta(pool03, 0, 4, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.conf.set(\"spark.sql.catalog.cosmosCatalog\", \"com.azure.cosmos.spark.CosmosCatalog\")\r\n",
        "spark.conf.set(\"spark.sql.catalog.cosmosCatalog.spark.cosmos.accountEndpoint\", cosmosEndpoint)\r\n",
        "spark.conf.set(\"spark.sql.catalog.cosmosCatalog.spark.cosmos.accountKey\", cosmosMasterKey)\r\n",
        "\r\n",
        "# create a cosmos database using catalog api\r\n",
        "spark.sql(\"CREATE DATABASE IF NOT EXISTS cosmosCatalog.{};\".format(cosmosDatabaseName))\r\n",
        "\r\n",
        "# create a cosmos container using catalog api\r\n",
        "spark.sql(\"CREATE TABLE IF NOT EXISTS cosmosCatalog.{}.{} using cosmos.oltp TBLPROPERTIES(partitionKeyPath = '/id', manualThroughput = '1100')\".format(cosmosDatabaseName, cosmosContainerName))"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "pool02",
              "session_id": 2,
              "statement_id": 41,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-05-04T22:50:23.774639Z",
              "session_start_time": null,
              "execution_start_time": "2022-05-04T22:50:23.8799994Z",
              "execution_finish_time": "2022-05-04T22:50:24.4699789Z"
            },
            "text/plain": "StatementMeta(pool02, 2, 41, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling o209.sql.\n: {\"ClassName\":\"CosmosException\",\"userAgent\":\"azsdk-java-cosmos/4.26.0-snapshot.1 Linux/4.15.0-1137-azure JRE/1.8.0_312\",\"statusCode\":403,\"resourceAddress\":\"https://mmharleycosmos.documents.azure.com:443/\",\"error\":\"{\\\"code\\\":\\\"Forbidden\\\",\\\"message\\\":\\\"Database Account mmharleycosmos does not exist\\\\r\\\\nActivityId: 45524590-ece2-4c71-81c4-f513be2582c4, Microsoft.Azure.Documents.Common/2.14.0, StatusCode: Forbidden\\\",\\\"additionalErrorInfo\\\":null}\",\"innerErrorMessage\":\"Database Account mmharleycosmos does not exist\\r\\nActivityId: 45524590-ece2-4c71-81c4-f513be2582c4, Microsoft.Azure.Documents.Common/2.14.0, StatusCode: Forbidden\",\"causeInfo\":null,\"responseHeaders\":\"{Transfer-Encoding=chunked, Strict-Transport-Security=max-age=31536000, Server=Microsoft-HTTPAPI/2.0, Content-Location=https://mmharleycosmos.documents.azure.com/, x-ms-gatewayversion=version=2.14.0, Date=Wed, 04 May 2022 22:50:23 GMT, x-ms-activity-id=45524590-ece2-4c71-81c4-f513be2582c4, Content-Type=application/json, x-ms-substatus=1008}\",\"requestHeaders\":\"[Accept=application/json, x-ms-date=Wed, 04 May 2022 22:50:24 GMT]\",\"cosmosDiagnostics\":{\"userAgent\":\"azsdk-java-cosmos/4.26.0-snapshot.1 Linux/4.15.0-1137-azure JRE/1.8.0_312\",\"activityId\":\"45524590-ece2-4c71-81c4-f513be2582c4\",\"requestLatencyInMs\":84,\"requestStartTimeUTC\":\"2022-05-04T22:50:24.090Z\",\"requestEndTimeUTC\":\"2022-05-04T22:50:24.174Z\",\"responseStatisticsList\":[],\"supplementalResponseStatisticsList\":[],\"addressResolutionStatistics\":{},\"regionsContacted\":[],\"retryContext\":{\"statusAndSubStatusCodes\":null,\"retryCount\":0,\"retryLatency\":0},\"metadataDiagnosticsContext\":{\"metadataDiagnosticList\":null},\"serializationDiagnosticsContext\":{\"serializationDiagnosticsList\":null},\"gatewayStatistics\":{\"sessionToken\":null,\"operationType\":\"Read\",\"resourceType\":\"DatabaseAccount\",\"statusCode\":403,\"subStatusCode\":1008,\"requestCharge\":\"0.0\",\"requestTimeline\":[{\"eventName\":\"connectionAcquired\",\"startTimeUTC\":\"2022-05-04T22:50:24.090Z\",\"durationInMicroSec\":1000},{\"eventName\":\"connectionConfigured\",\"startTimeUTC\":\"2022-05-04T22:50:24.091Z\",\"durationInMicroSec\":0},{\"eventName\":\"requestSent\",\"startTimeUTC\":\"2022-05-04T22:50:24.091Z\",\"durationInMicroSec\":0},{\"eventName\":\"transitTime\",\"startTimeUTC\":\"2022-05-04T22:50:24.091Z\",\"durationInMicroSec\":82000},{\"eventName\":\"received\",\"startTimeUTC\":\"2022-05-04T22:50:24.173Z\",\"durationInMicroSec\":1000}],\"partitionKeyRangeId\":null},\"systemInformation\":{\"usedMemory\":\"1438505 KB\",\"availableMemory\":\"50757335 KB\",\"systemCpuLoad\":\"(2022-05-04T22:49:58.001Z 3.8%), (2022-05-04T22:50:03.001Z 4.0%), (2022-05-04T22:50:08.001Z 4.0%), (2022-05-04T22:50:13.001Z 6.5%), (2022-05-04T22:50:18.001Z 4.0%), (2022-05-04T22:50:23.001Z 4.5%)\",\"availableProcessors\":8},\"clientCfgs\":{\"id\":0,\"connectionMode\":\"DIRECT\",\"numberOfClients\":1,\"connCfg\":{\"rntbd\":\"(cto:PT10S, nrto:PT10S, icto:PT0S, ieto:PT1H, mcpe:130, mrpc:30, cer:false)\",\"gw\":\"(cps:1000, nrto:null, icto:null, p:false)\",\"other\":\"(ed: true, cs: false)\"},\"consistencyCfg\":\"(consistency: Eventual, mm: true, prgns: [])\"}}}\n\tat azure_cosmos_spark.com.azure.cosmos.BridgeInternal.createCosmosException(BridgeInternal.java:474)\n\tat azure_cosmos_spark.com.azure.cosmos.implementation.RxGatewayStoreModel.validateOrThrow(RxGatewayStoreModel.java:440)\n\tat azure_cosmos_spark.com.azure.cosmos.implementation.RxGatewayStoreModel.lambda$toDocumentServiceResponse$0(RxGatewayStoreModel.java:347)\n\tat azure_cosmos_spark.reactor.core.publisher.FluxMap$MapSubscriber.onNext(FluxMap.java:106)\n\tat azure_cosmos_spark.reactor.core.publisher.FluxSwitchIfEmpty$SwitchIfEmptySubscriber.onNext(FluxSwitchIfEmpty.java:74)\n\tat azure_cosmos_spark.reactor.core.publisher.FluxPeek$PeekSubscriber.onNext(FluxPeek.java:200)\n\tat azure_cosmos_spark.reactor.core.publisher.FluxHandle$HandleSubscriber.onNext(FluxHandle.java:119)\n\tat azure_cosmos_spark.reactor.core.publisher.FluxMap$MapConditionalSubscriber.onNext(FluxMap.java:220)\n\tat azure_cosmos_spark.reactor.core.publisher.FluxDoFinally$DoFinallySubscriber.onNext(FluxDoFinally.java:130)\n\tat azure_cosmos_spark.reactor.core.publisher.FluxHandleFuseable$HandleFuseableSubscriber.onNext(FluxHandleFuseable.java:184)\n\tat azure_cosmos_spark.reactor.core.publisher.FluxContextWrite$ContextWriteSubscriber.onNext(FluxContextWrite.java:107)\n\tat azure_cosmos_spark.reactor.core.publisher.Operators$MonoSubscriber.complete(Operators.java:1816)\n\tat azure_cosmos_spark.reactor.core.publisher.MonoCollectList$MonoCollectListSubscriber.onComplete(MonoCollectList.java:128)\n\tat azure_cosmos_spark.reactor.core.publisher.FluxPeek$PeekSubscriber.onComplete(FluxPeek.java:260)\n\tat azure_cosmos_spark.reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:142)\n\tat azure_cosmos_spark.reactor.netty.channel.FluxReceive.onInboundComplete(FluxReceive.java:400)\n\tat azure_cosmos_spark.reactor.netty.channel.ChannelOperations.onInboundComplete(ChannelOperations.java:419)\n\tat azure_cosmos_spark.reactor.netty.channel.ChannelOperations.terminate(ChannelOperations.java:473)\n\tat azure_cosmos_spark.reactor.netty.http.client.HttpClientOperations.onInboundNext(HttpClientOperations.java:702)\n\tat azure_cosmos_spark.reactor.netty.channel.ChannelOperationsHandler.channelRead(ChannelOperationsHandler.java:93)\n\tat azure_cosmos_spark.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat azure_cosmos_spark.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat azure_cosmos_spark.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat azure_cosmos_spark.io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\n\tat azure_cosmos_spark.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat azure_cosmos_spark.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat azure_cosmos_spark.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat azure_cosmos_spark.io.netty.channel.CombinedChannelDuplexHandler$DelegatingChannelHandlerContext.fireChannelRead(CombinedChannelDuplexHandler.java:436)\n\tat azure_cosmos_spark.io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:324)\n\tat azure_cosmos_spark.io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:296)\n\tat azure_cosmos_spark.io.netty.channel.CombinedChannelDuplexHandler.channelRead(CombinedChannelDuplexHandler.java:251)\n\tat azure_cosmos_spark.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat azure_cosmos_spark.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat azure_cosmos_spark.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat azure_cosmos_spark.io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:1371)\n\tat azure_cosmos_spark.io.netty.handler.ssl.SslHandler.decodeJdkCompatible(SslHandler.java:1234)\n\tat azure_cosmos_spark.io.netty.handler.ssl.SslHandler.decode(SslHandler.java:1283)\n\tat azure_cosmos_spark.io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:507)\n\tat azure_cosmos_spark.io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:446)\n\tat azure_cosmos_spark.io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:276)\n\tat azure_cosmos_spark.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat azure_cosmos_spark.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat azure_cosmos_spark.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat azure_cosmos_spark.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\n\tat azure_cosmos_spark.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat azure_cosmos_spark.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat azure_cosmos_spark.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\n\tat azure_cosmos_spark.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)\n\tat azure_cosmos_spark.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:722)\n\tat azure_cosmos_spark.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:658)\n\tat azure_cosmos_spark.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:584)\n\tat azure_cosmos_spark.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:496)\n\tat azure_cosmos_spark.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\n\tat azure_cosmos_spark.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\tat azure_cosmos_spark.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\tat java.lang.Thread.run(Thread.java:748)\n\tSuppressed: java.lang.Exception: #block terminated with an error\n\t\tat azure_cosmos_spark.reactor.core.publisher.BlockingSingleSubscriber.blockingGet(BlockingSingleSubscriber.java:99)\n\t\tat azure_cosmos_spark.reactor.core.publisher.Mono.block(Mono.java:1707)\n\t\tat com.azure.cosmos.spark.CosmosCatalog.$anonfun$loadNamespaceMetadataImpl$1(CosmosCatalog.scala:178)\n\t\tat com.azure.cosmos.spark.Loan$Loan.to(Using.scala:43)\n\t\tat com.azure.cosmos.spark.CosmosCatalog.loadNamespaceMetadataImpl(CosmosCatalog.scala:175)\n\t\tat com.azure.cosmos.spark.CosmosCatalog.$anonfun$loadNamespaceMetadata$1(CosmosCatalog.scala:162)\n\t\tat com.azure.cosmos.spark.TransientErrorsRetryPolicy$.$anonfun$executeWithRetry$1(TransientErrorsRetryPolicy.scala:32)\n\t\tat scala.util.control.Breaks.breakable(Breaks.scala:42)\n\t\tat com.azure.cosmos.spark.TransientErrorsRetryPolicy$.executeWithRetry(TransientErrorsRetryPolicy.scala:26)\n\t\tat com.azure.cosmos.spark.CosmosCatalog.loadNamespaceMetadata(CosmosCatalog.scala:162)\n\t\tat org.apache.spark.sql.connector.catalog.SupportsNamespaces.namespaceExists(SupportsNamespaces.java:97)\n\t\tat com.azure.cosmos.spark.CosmosCatalog.namespaceExists(CosmosCatalog.scala:48)\n\t\tat org.apache.spark.sql.execution.datasources.v2.CreateNamespaceExec.run(CreateNamespaceExec.scala:42)\n\t\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:40)\n\t\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:40)\n\t\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:46)\n\t\tat org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:228)\n\t\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\n\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:107)\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:181)\n\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:94)\n\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n\t\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\n\t\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:228)\n\t\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\t\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)\n\t\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:618)\n\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\t\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:613)\n\t\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\t\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\t\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\t\tat java.lang.reflect.Method.invoke(Method.java:498)\n\t\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\t\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\t\tat py4j.Gateway.invoke(Gateway.java:282)\n\t\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\t\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\t\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\t\t... 1 more\n",
          "traceback": [
            "Py4JJavaError: An error occurred while calling o209.sql.\n: {\"ClassName\":\"CosmosException\",\"userAgent\":\"azsdk-java-cosmos/4.26.0-snapshot.1 Linux/4.15.0-1137-azure JRE/1.8.0_312\",\"statusCode\":403,\"resourceAddress\":\"https://mmharleycosmos.documents.azure.com:443/\",\"error\":\"{\\\"code\\\":\\\"Forbidden\\\",\\\"message\\\":\\\"Database Account mmharleycosmos does not exist\\\\r\\\\nActivityId: 45524590-ece2-4c71-81c4-f513be2582c4, Microsoft.Azure.Documents.Common/2.14.0, StatusCode: Forbidden\\\",\\\"additionalErrorInfo\\\":null}\",\"innerErrorMessage\":\"Database Account mmharleycosmos does not exist\\r\\nActivityId: 45524590-ece2-4c71-81c4-f513be2582c4, Microsoft.Azure.Documents.Common/2.14.0, StatusCode: Forbidden\",\"causeInfo\":null,\"responseHeaders\":\"{Transfer-Encoding=chunked, Strict-Transport-Security=max-age=31536000, Server=Microsoft-HTTPAPI/2.0, Content-Location=https://mmharleycosmos.documents.azure.com/, x-ms-gatewayversion=version=2.14.0, Date=Wed, 04 May 2022 22:50:23 GMT, x-ms-activity-id=45524590-ece2-4c71-81c4-f513be2582c4, Content-Type=application/json, x-ms-substatus=1008}\",\"requestHeaders\":\"[Accept=application/json, x-ms-date=Wed, 04 May 2022 22:50:24 GMT]\",\"cosmosDiagnostics\":{\"userAgent\":\"azsdk-java-cosmos/4.26.0-snapshot.1 Linux/4.15.0-1137-azure JRE/1.8.0_312\",\"activityId\":\"45524590-ece2-4c71-81c4-f513be2582c4\",\"requestLatencyInMs\":84,\"requestStartTimeUTC\":\"2022-05-04T22:50:24.090Z\",\"requestEndTimeUTC\":\"2022-05-04T22:50:24.174Z\",\"responseStatisticsList\":[],\"supplementalResponseStatisticsList\":[],\"addressResolutionStatistics\":{},\"regionsContacted\":[],\"retryContext\":{\"statusAndSubStatusCodes\":null,\"retryCount\":0,\"retryLatency\":0},\"metadataDiagnosticsContext\":{\"metadataDiagnosticList\":null},\"serializationDiagnosticsContext\":{\"serializationDiagnosticsList\":null},\"gatewayStatistics\":{\"sessionToken\":null,\"operationType\":\"Read\",\"resourceType\":\"DatabaseAccount\",\"statusCode\":403,\"subStatusCode\":1008,\"requestCharge\":\"0.0\",\"requestTimeline\":[{\"eventName\":\"connectionAcquired\",\"startTimeUTC\":\"2022-05-04T22:50:24.090Z\",\"durationInMicroSec\":1000},{\"eventName\":\"connectionConfigured\",\"startTimeUTC\":\"2022-05-04T22:50:24.091Z\",\"durationInMicroSec\":0},{\"eventName\":\"requestSent\",\"startTimeUTC\":\"2022-05-04T22:50:24.091Z\",\"durationInMicroSec\":0},{\"eventName\":\"transitTime\",\"startTimeUTC\":\"2022-05-04T22:50:24.091Z\",\"durationInMicroSec\":82000},{\"eventName\":\"received\",\"startTimeUTC\":\"2022-05-04T22:50:24.173Z\",\"durationInMicroSec\":1000}],\"partitionKeyRangeId\":null},\"systemInformation\":{\"usedMemory\":\"1438505 KB\",\"availableMemory\":\"50757335 KB\",\"systemCpuLoad\":\"(2022-05-04T22:49:58.001Z 3.8%), (2022-05-04T22:50:03.001Z 4.0%), (2022-05-04T22:50:08.001Z 4.0%), (2022-05-04T22:50:13.001Z 6.5%), (2022-05-04T22:50:18.001Z 4.0%), (2022-05-04T22:50:23.001Z 4.5%)\",\"availableProcessors\":8},\"clientCfgs\":{\"id\":0,\"connectionMode\":\"DIRECT\",\"numberOfClients\":1,\"connCfg\":{\"rntbd\":\"(cto:PT10S, nrto:PT10S, icto:PT0S, ieto:PT1H, mcpe:130, mrpc:30, cer:false)\",\"gw\":\"(cps:1000, nrto:null, icto:null, p:false)\",\"other\":\"(ed: true, cs: false)\"},\"consistencyCfg\":\"(consistency: Eventual, mm: true, prgns: [])\"}}}\n\tat azure_cosmos_spark.com.azure.cosmos.BridgeInternal.createCosmosException(BridgeInternal.java:474)\n\tat azure_cosmos_spark.com.azure.cosmos.implementation.RxGatewayStoreModel.validateOrThrow(RxGatewayStoreModel.java:440)\n\tat azure_cosmos_spark.com.azure.cosmos.implementation.RxGatewayStoreModel.lambda$toDocumentServiceResponse$0(RxGatewayStoreModel.java:347)\n\tat azure_cosmos_spark.reactor.core.publisher.FluxMap$MapSubscriber.onNext(FluxMap.java:106)\n\tat azure_cosmos_spark.reactor.core.publisher.FluxSwitchIfEmpty$SwitchIfEmptySubscriber.onNext(FluxSwitchIfEmpty.java:74)\n\tat azure_cosmos_spark.reactor.core.publisher.FluxPeek$PeekSubscriber.onNext(FluxPeek.java:200)\n\tat azure_cosmos_spark.reactor.core.publisher.FluxHandle$HandleSubscriber.onNext(FluxHandle.java:119)\n\tat azure_cosmos_spark.reactor.core.publisher.FluxMap$MapConditionalSubscriber.onNext(FluxMap.java:220)\n\tat azure_cosmos_spark.reactor.core.publisher.FluxDoFinally$DoFinallySubscriber.onNext(FluxDoFinally.java:130)\n\tat azure_cosmos_spark.reactor.core.publisher.FluxHandleFuseable$HandleFuseableSubscriber.onNext(FluxHandleFuseable.java:184)\n\tat azure_cosmos_spark.reactor.core.publisher.FluxContextWrite$ContextWriteSubscriber.onNext(FluxContextWrite.java:107)\n\tat azure_cosmos_spark.reactor.core.publisher.Operators$MonoSubscriber.complete(Operators.java:1816)\n\tat azure_cosmos_spark.reactor.core.publisher.MonoCollectList$MonoCollectListSubscriber.onComplete(MonoCollectList.java:128)\n\tat azure_cosmos_spark.reactor.core.publisher.FluxPeek$PeekSubscriber.onComplete(FluxPeek.java:260)\n\tat azure_cosmos_spark.reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:142)\n\tat azure_cosmos_spark.reactor.netty.channel.FluxReceive.onInboundComplete(FluxReceive.java:400)\n\tat azure_cosmos_spark.reactor.netty.channel.ChannelOperations.onInboundComplete(ChannelOperations.java:419)\n\tat azure_cosmos_spark.reactor.netty.channel.ChannelOperations.terminate(ChannelOperations.java:473)\n\tat azure_cosmos_spark.reactor.netty.http.client.HttpClientOperations.onInboundNext(HttpClientOperations.java:702)\n\tat azure_cosmos_spark.reactor.netty.channel.ChannelOperationsHandler.channelRead(ChannelOperationsHandler.java:93)\n\tat azure_cosmos_spark.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat azure_cosmos_spark.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat azure_cosmos_spark.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat azure_cosmos_spark.io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\n\tat azure_cosmos_spark.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat azure_cosmos_spark.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat azure_cosmos_spark.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat azure_cosmos_spark.io.netty.channel.CombinedChannelDuplexHandler$DelegatingChannelHandlerContext.fireChannelRead(CombinedChannelDuplexHandler.java:436)\n\tat azure_cosmos_spark.io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:324)\n\tat azure_cosmos_spark.io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:296)\n\tat azure_cosmos_spark.io.netty.channel.CombinedChannelDuplexHandler.channelRead(CombinedChannelDuplexHandler.java:251)\n\tat azure_cosmos_spark.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat azure_cosmos_spark.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat azure_cosmos_spark.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat azure_cosmos_spark.io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:1371)\n\tat azure_cosmos_spark.io.netty.handler.ssl.SslHandler.decodeJdkCompatible(SslHandler.java:1234)\n\tat azure_cosmos_spark.io.netty.handler.ssl.SslHandler.decode(SslHandler.java:1283)\n\tat azure_cosmos_spark.io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:507)\n\tat azure_cosmos_spark.io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:446)\n\tat azure_cosmos_spark.io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:276)\n\tat azure_cosmos_spark.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat azure_cosmos_spark.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat azure_cosmos_spark.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat azure_cosmos_spark.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\n\tat azure_cosmos_spark.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat azure_cosmos_spark.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat azure_cosmos_spark.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\n\tat azure_cosmos_spark.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)\n\tat azure_cosmos_spark.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:722)\n\tat azure_cosmos_spark.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:658)\n\tat azure_cosmos_spark.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:584)\n\tat azure_cosmos_spark.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:496)\n\tat azure_cosmos_spark.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\n\tat azure_cosmos_spark.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\tat azure_cosmos_spark.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\tat java.lang.Thread.run(Thread.java:748)\n\tSuppressed: java.lang.Exception: #block terminated with an error\n\t\tat azure_cosmos_spark.reactor.core.publisher.BlockingSingleSubscriber.blockingGet(BlockingSingleSubscriber.java:99)\n\t\tat azure_cosmos_spark.reactor.core.publisher.Mono.block(Mono.java:1707)\n\t\tat com.azure.cosmos.spark.CosmosCatalog.$anonfun$loadNamespaceMetadataImpl$1(CosmosCatalog.scala:178)\n\t\tat com.azure.cosmos.spark.Loan$Loan.to(Using.scala:43)\n\t\tat com.azure.cosmos.spark.CosmosCatalog.loadNamespaceMetadataImpl(CosmosCatalog.scala:175)\n\t\tat com.azure.cosmos.spark.CosmosCatalog.$anonfun$loadNamespaceMetadata$1(CosmosCatalog.scala:162)\n\t\tat com.azure.cosmos.spark.TransientErrorsRetryPolicy$.$anonfun$executeWithRetry$1(TransientErrorsRetryPolicy.scala:32)\n\t\tat scala.util.control.Breaks.breakable(Breaks.scala:42)\n\t\tat com.azure.cosmos.spark.TransientErrorsRetryPolicy$.executeWithRetry(TransientErrorsRetryPolicy.scala:26)\n\t\tat com.azure.cosmos.spark.CosmosCatalog.loadNamespaceMetadata(CosmosCatalog.scala:162)\n\t\tat org.apache.spark.sql.connector.catalog.SupportsNamespaces.namespaceExists(SupportsNamespaces.java:97)\n\t\tat com.azure.cosmos.spark.CosmosCatalog.namespaceExists(CosmosCatalog.scala:48)\n\t\tat org.apache.spark.sql.execution.datasources.v2.CreateNamespaceExec.run(CreateNamespaceExec.scala:42)\n\t\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:40)\n\t\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:40)\n\t\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:46)\n\t\tat org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:228)\n\t\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\n\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:107)\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:181)\n\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:94)\n\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n\t\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\n\t\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:228)\n\t\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\t\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)\n\t\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:618)\n\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\t\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:613)\n\t\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\t\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\t\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\t\tat java.lang.reflect.Method.invoke(Method.java:498)\n\t\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\t\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\t\tat py4j.Gateway.invoke(Gateway.java:282)\n\t\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\t\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\t\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\t\t... 1 more\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/sql/session.py\", line 723, in sql\n    return DataFrame(self._jsparkSession.sql(sqlQuery), self._wrapped)\n",
            "  File \"/home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1304, in __call__\n    return_value = get_return_value(\n",
            "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 111, in deco\n    return f(*a, **kw)\n",
            "  File \"/home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages/py4j/protocol.py\", line 326, in get_return_value\n    raise Py4JJavaError(\n",
            "py4j.protocol.Py4JJavaError: An error occurred while calling o209.sql.\n: {\"ClassName\":\"CosmosException\",\"userAgent\":\"azsdk-java-cosmos/4.26.0-snapshot.1 Linux/4.15.0-1137-azure JRE/1.8.0_312\",\"statusCode\":403,\"resourceAddress\":\"https://mmharleycosmos.documents.azure.com:443/\",\"error\":\"{\\\"code\\\":\\\"Forbidden\\\",\\\"message\\\":\\\"Database Account mmharleycosmos does not exist\\\\r\\\\nActivityId: 45524590-ece2-4c71-81c4-f513be2582c4, Microsoft.Azure.Documents.Common/2.14.0, StatusCode: Forbidden\\\",\\\"additionalErrorInfo\\\":null}\",\"innerErrorMessage\":\"Database Account mmharleycosmos does not exist\\r\\nActivityId: 45524590-ece2-4c71-81c4-f513be2582c4, Microsoft.Azure.Documents.Common/2.14.0, StatusCode: Forbidden\",\"causeInfo\":null,\"responseHeaders\":\"{Transfer-Encoding=chunked, Strict-Transport-Security=max-age=31536000, Server=Microsoft-HTTPAPI/2.0, Content-Location=https://mmharleycosmos.documents.azure.com/, x-ms-gatewayversion=version=2.14.0, Date=Wed, 04 May 2022 22:50:23 GMT, x-ms-activity-id=45524590-ece2-4c71-81c4-f513be2582c4, Content-Type=application/json, x-ms-substatus=1008}\",\"requestHeaders\":\"[Accept=application/json, x-ms-date=Wed, 04 May 2022 22:50:24 GMT]\",\"cosmosDiagnostics\":{\"userAgent\":\"azsdk-java-cosmos/4.26.0-snapshot.1 Linux/4.15.0-1137-azure JRE/1.8.0_312\",\"activityId\":\"45524590-ece2-4c71-81c4-f513be2582c4\",\"requestLatencyInMs\":84,\"requestStartTimeUTC\":\"2022-05-04T22:50:24.090Z\",\"requestEndTimeUTC\":\"2022-05-04T22:50:24.174Z\",\"responseStatisticsList\":[],\"supplementalResponseStatisticsList\":[],\"addressResolutionStatistics\":{},\"regionsContacted\":[],\"retryContext\":{\"statusAndSubStatusCodes\":null,\"retryCount\":0,\"retryLatency\":0},\"metadataDiagnosticsContext\":{\"metadataDiagnosticList\":null},\"serializationDiagnosticsContext\":{\"serializationDiagnosticsList\":null},\"gatewayStatistics\":{\"sessionToken\":null,\"operationType\":\"Read\",\"resourceType\":\"DatabaseAccount\",\"statusCode\":403,\"subStatusCode\":1008,\"requestCharge\":\"0.0\",\"requestTimeline\":[{\"eventName\":\"connectionAcquired\",\"startTimeUTC\":\"2022-05-04T22:50:24.090Z\",\"durationInMicroSec\":1000},{\"eventName\":\"connectionConfigured\",\"startTimeUTC\":\"2022-05-04T22:50:24.091Z\",\"durationInMicroSec\":0},{\"eventName\":\"requestSent\",\"startTimeUTC\":\"2022-05-04T22:50:24.091Z\",\"durationInMicroSec\":0},{\"eventName\":\"transitTime\",\"startTimeUTC\":\"2022-05-04T22:50:24.091Z\",\"durationInMicroSec\":82000},{\"eventName\":\"received\",\"startTimeUTC\":\"2022-05-04T22:50:24.173Z\",\"durationInMicroSec\":1000}],\"partitionKeyRangeId\":null},\"systemInformation\":{\"usedMemory\":\"1431426 KB\",\"availableMemory\":\"50764414 KB\",\"systemCpuLoad\":\"(2022-05-04T22:49:58.001Z 3.8%), (2022-05-04T22:50:03.001Z 4.0%), (2022-05-04T22:50:08.001Z 4.0%), (2022-05-04T22:50:13.001Z 6.5%), (2022-05-04T22:50:18.001Z 4.0%), (2022-05-04T22:50:23.001Z 4.5%)\",\"availableProcessors\":8},\"clientCfgs\":{\"id\":0,\"connectionMode\":\"DIRECT\",\"numberOfClients\":1,\"connCfg\":{\"rntbd\":\"(cto:PT10S, nrto:PT10S, icto:PT0S, ieto:PT1H, mcpe:130, mrpc:30, cer:false)\",\"gw\":\"(cps:1000, nrto:null, icto:null, p:false)\",\"other\":\"(ed: true, cs: false)\"},\"consistencyCfg\":\"(consistency: Eventual, mm: true, prgns: [])\"}}}\n\tat azure_cosmos_spark.com.azure.cosmos.BridgeInternal.createCosmosException(BridgeInternal.java:474)\n\tat azure_cosmos_spark.com.azure.cosmos.implementation.RxGatewayStoreModel.validateOrThrow(RxGatewayStoreModel.java:440)\n\tat azure_cosmos_spark.com.azure.cosmos.implementation.RxGatewayStoreModel.lambda$toDocumentServiceResponse$0(RxGatewayStoreModel.java:347)\n\tat azure_cosmos_spark.reactor.core.publisher.FluxMap$MapSubscriber.onNext(FluxMap.java:106)\n\tat azure_cosmos_spark.reactor.core.publisher.FluxSwitchIfEmpty$SwitchIfEmptySubscriber.onNext(FluxSwitchIfEmpty.java:74)\n\tat azure_cosmos_spark.reactor.core.publisher.FluxPeek$PeekSubscriber.onNext(FluxPeek.java:200)\n\tat azure_cosmos_spark.reactor.core.publisher.FluxHandle$HandleSubscriber.onNext(FluxHandle.java:119)\n\tat azure_cosmos_spark.reactor.core.publisher.FluxMap$MapConditionalSubscriber.onNext(FluxMap.java:220)\n\tat azure_cosmos_spark.reactor.core.publisher.FluxDoFinally$DoFinallySubscriber.onNext(FluxDoFinally.java:130)\n\tat azure_cosmos_spark.reactor.core.publisher.FluxHandleFuseable$HandleFuseableSubscriber.onNext(FluxHandleFuseable.java:184)\n\tat azure_cosmos_spark.reactor.core.publisher.FluxContextWrite$ContextWriteSubscriber.onNext(FluxContextWrite.java:107)\n\tat azure_cosmos_spark.reactor.core.publisher.Operators$MonoSubscriber.complete(Operators.java:1816)\n\tat azure_cosmos_spark.reactor.core.publisher.MonoCollectList$MonoCollectListSubscriber.onComplete(MonoCollectList.java:128)\n\tat azure_cosmos_spark.reactor.core.publisher.FluxPeek$PeekSubscriber.onComplete(FluxPeek.java:260)\n\tat azure_cosmos_spark.reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:142)\n\tat azure_cosmos_spark.reactor.netty.channel.FluxReceive.onInboundComplete(FluxReceive.java:400)\n\tat azure_cosmos_spark.reactor.netty.channel.ChannelOperations.onInboundComplete(ChannelOperations.java:419)\n\tat azure_cosmos_spark.reactor.netty.channel.ChannelOperations.terminate(ChannelOperations.java:473)\n\tat azure_cosmos_spark.reactor.netty.http.client.HttpClientOperations.onInboundNext(HttpClientOperations.java:702)\n\tat azure_cosmos_spark.reactor.netty.channel.ChannelOperationsHandler.channelRead(ChannelOperationsHandler.java:93)\n\tat azure_cosmos_spark.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat azure_cosmos_spark.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat azure_cosmos_spark.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat azure_cosmos_spark.io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\n\tat azure_cosmos_spark.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat azure_cosmos_spark.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat azure_cosmos_spark.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat azure_cosmos_spark.io.netty.channel.CombinedChannelDuplexHandler$DelegatingChannelHandlerContext.fireChannelRead(CombinedChannelDuplexHandler.java:436)\n\tat azure_cosmos_spark.io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:324)\n\tat azure_cosmos_spark.io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:296)\n\tat azure_cosmos_spark.io.netty.channel.CombinedChannelDuplexHandler.channelRead(CombinedChannelDuplexHandler.java:251)\n\tat azure_cosmos_spark.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat azure_cosmos_spark.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat azure_cosmos_spark.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat azure_cosmos_spark.io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:1371)\n\tat azure_cosmos_spark.io.netty.handler.ssl.SslHandler.decodeJdkCompatible(SslHandler.java:1234)\n\tat azure_cosmos_spark.io.netty.handler.ssl.SslHandler.decode(SslHandler.java:1283)\n\tat azure_cosmos_spark.io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:507)\n\tat azure_cosmos_spark.io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:446)\n\tat azure_cosmos_spark.io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:276)\n\tat azure_cosmos_spark.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat azure_cosmos_spark.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat azure_cosmos_spark.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat azure_cosmos_spark.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\n\tat azure_cosmos_spark.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat azure_cosmos_spark.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat azure_cosmos_spark.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\n\tat azure_cosmos_spark.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)\n\tat azure_cosmos_spark.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:722)\n\tat azure_cosmos_spark.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:658)\n\tat azure_cosmos_spark.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:584)\n\tat azure_cosmos_spark.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:496)\n\tat azure_cosmos_spark.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\n\tat azure_cosmos_spark.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\tat azure_cosmos_spark.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\tat java.lang.Thread.run(Thread.java:748)\n\tSuppressed: java.lang.Exception: #block terminated with an error\n\t\tat azure_cosmos_spark.reactor.core.publisher.BlockingSingleSubscriber.blockingGet(BlockingSingleSubscriber.java:99)\n\t\tat azure_cosmos_spark.reactor.core.publisher.Mono.block(Mono.java:1707)\n\t\tat com.azure.cosmos.spark.CosmosCatalog.$anonfun$loadNamespaceMetadataImpl$1(CosmosCatalog.scala:178)\n\t\tat com.azure.cosmos.spark.Loan$Loan.to(Using.scala:43)\n\t\tat com.azure.cosmos.spark.CosmosCatalog.loadNamespaceMetadataImpl(CosmosCatalog.scala:175)\n\t\tat com.azure.cosmos.spark.CosmosCatalog.$anonfun$loadNamespaceMetadata$1(CosmosCatalog.scala:162)\n\t\tat com.azure.cosmos.spark.TransientErrorsRetryPolicy$.$anonfun$executeWithRetry$1(TransientErrorsRetryPolicy.scala:32)\n\t\tat scala.util.control.Breaks.breakable(Breaks.scala:42)\n\t\tat com.azure.cosmos.spark.TransientErrorsRetryPolicy$.executeWithRetry(TransientErrorsRetryPolicy.scala:26)\n\t\tat com.azure.cosmos.spark.CosmosCatalog.loadNamespaceMetadata(CosmosCatalog.scala:162)\n\t\tat org.apache.spark.sql.connector.catalog.SupportsNamespaces.namespaceExists(SupportsNamespaces.java:97)\n\t\tat com.azure.cosmos.spark.CosmosCatalog.namespaceExists(CosmosCatalog.scala:48)\n\t\tat org.apache.spark.sql.execution.datasources.v2.CreateNamespaceExec.run(CreateNamespaceExec.scala:42)\n\t\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:40)\n\t\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:40)\n\t\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:46)\n\t\tat org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:228)\n\t\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\n\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:107)\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:181)\n\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:94)\n\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n\t\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\n\t\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:228)\n\t\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\t\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)\n\t\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:618)\n\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\t\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:613)\n\t\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\t\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\t\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\t\tat java.lang.reflect.Method.invoke(Method.java:498)\n\t\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\t\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\t\tat py4j.Gateway.invoke(Gateway.java:282)\n\t\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\t\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\t\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\t\t... 1 more\n\n"
          ]
        }
      ],
      "execution_count": 40,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.createDataFrame(((\"cat-alive\", \"Schrodinger cat\", 2, True), (\"cat-dead\", \"Schrodinger cat\", 2, False)))\\\r\n",
        "    .toDF(\"id\",\"name\",\"age\",\"isAlive\")\r\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "pool02",
              "session_id": 2,
              "statement_id": 23,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-05-04T22:28:07.6029315Z",
              "session_start_time": null,
              "execution_start_time": "2022-05-04T22:28:07.72079Z",
              "execution_finish_time": "2022-05-04T22:28:07.8716822Z"
            },
            "text/plain": "StatementMeta(pool02, 2, 23, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 22,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display(df)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "pool02",
              "session_id": 2,
              "statement_id": 24,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-05-04T22:28:08.5702927Z",
              "session_start_time": null,
              "execution_start_time": "2022-05-04T22:28:08.6844343Z",
              "execution_finish_time": "2022-05-04T22:28:09.1816402Z"
            },
            "text/plain": "StatementMeta(pool02, 2, 24, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.synapse.widget-view+json": {
              "widget_id": "21b209de-9122-48c5-b0cd-b9fa8b5ea788",
              "widget_type": "Synapse.DataFrame"
            },
            "text/plain": "SynapseWidget(Synapse.DataFrame, 21b209de-9122-48c5-b0cd-b9fa8b5ea788)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 23,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df\\\r\n",
        "   .write\\\r\n",
        "   .format(\"cosmos.oltp\")\\\r\n",
        "   .options(**cfg)\\\r\n",
        "   .mode(\"APPEND\")\\\r\n",
        "   .save()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "pool02",
              "session_id": 2,
              "statement_id": 25,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-05-04T22:28:10.19275Z",
              "session_start_time": null,
              "execution_start_time": "2022-05-04T22:28:10.3339112Z",
              "execution_finish_time": "2022-05-04T22:28:12.128162Z"
            },
            "text/plain": "StatementMeta(pool02, 2, 25, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 24,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\r\n",
        "\r\n",
        "df = spark.read.format(\"cosmos.oltp\").options(**cfg)\\\r\n",
        " .option(\"spark.cosmos.read.inferSchema.enabled\", \"true\")\\\r\n",
        " .load()\r\n",
        "\r\n",
        "df.filter(col(\"isAlive\") == True)\\\r\n",
        " .show()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "pool02",
              "session_id": 2,
              "statement_id": 19,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-05-04T22:27:31.2057234Z",
              "session_start_time": null,
              "execution_start_time": "2022-05-04T22:27:32.118456Z",
              "execution_finish_time": "2022-05-04T22:27:33.8872386Z"
            },
            "text/plain": "StatementMeta(pool02, 2, 19, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---------------+---+-------+\n|       id|           name|age|isAlive|\n+---------+---------------+---+-------+\n|cat-alive|Schrodinger cat|  2|   true|\n+---------+---------------+---+-------+"
          ]
        }
      ],
      "execution_count": 18,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark.sql.functions as f\r\n",
        "df_to_update = df.withColumn('name', f.lit('newname'))"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "pool02",
              "session_id": 2,
              "statement_id": 26,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-05-04T22:28:21.8989923Z",
              "session_start_time": null,
              "execution_start_time": "2022-05-04T22:28:22.0144855Z",
              "execution_finish_time": "2022-05-04T22:28:22.1755376Z"
            },
            "text/plain": "StatementMeta(pool02, 2, 26, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 25,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_to_update \\\r\n",
        "        .write \\\r\n",
        "        .format(\"cosmos.oltp\") \\\r\n",
        "        .mode(\"Append\") \\\r\n",
        "        .options(**cfg) \\\r\n",
        "        .save()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "pool02",
              "session_id": 2,
              "statement_id": 27,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-05-04T22:28:22.9037573Z",
              "session_start_time": null,
              "execution_start_time": "2022-05-04T22:28:23.016091Z",
              "execution_finish_time": "2022-05-04T22:28:24.9372648Z"
            },
            "text/plain": "StatementMeta(pool02, 2, 27, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 26,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.format(\"cosmos.oltp\").options(**cfg)\\\r\n",
        " .option(\"spark.cosmos.read.inferSchema.enabled\", \"true\")\\\r\n",
        " .load().show()\r\n",
        "\r\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "pool02",
              "session_id": 2,
              "statement_id": 28,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-05-04T22:28:25.9045026Z",
              "session_start_time": null,
              "execution_start_time": "2022-05-04T22:28:26.0078044Z",
              "execution_finish_time": "2022-05-04T22:28:27.8508843Z"
            },
            "text/plain": "StatementMeta(pool02, 2, 28, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-------+---+-------+\n|       id|   name|age|isAlive|\n+---------+-------+---+-------+\n|cat-alive|newname|  2|   true|\n| cat-dead|newname|  2|  false|\n+---------+-------+---+-------+"
          ]
        }
      ],
      "execution_count": 27,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {
        "21b209de-9122-48c5-b0cd-b9fa8b5ea788": {
          "type": "Synapse.DataFrame",
          "sync_state": {
            "table": {
              "rows": [
                {
                  "0": "cat-alive",
                  "1": "Schrodinger cat",
                  "2": "2",
                  "3": "true"
                },
                {
                  "0": "cat-dead",
                  "1": "Schrodinger cat",
                  "2": "2",
                  "3": "false"
                }
              ],
              "schema": [
                {
                  "key": "0",
                  "name": "id",
                  "type": "string"
                },
                {
                  "key": "1",
                  "name": "name",
                  "type": "string"
                },
                {
                  "key": "2",
                  "name": "age",
                  "type": "bigint"
                },
                {
                  "key": "3",
                  "name": "isAlive",
                  "type": "boolean"
                }
              ],
              "truncated": false
            },
            "isSummary": false,
            "language": "scala"
          },
          "persist_state": {
            "view": {
              "type": "details",
              "tableOptions": {},
              "chartOptions": {
                "chartType": "bar",
                "aggregationType": "sum",
                "categoryFieldKeys": [
                  "0"
                ],
                "seriesFieldKeys": [
                  "2"
                ],
                "isStacked": false
              }
            }
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}